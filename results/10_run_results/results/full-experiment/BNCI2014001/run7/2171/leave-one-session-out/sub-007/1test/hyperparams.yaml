# Generated 2024-04-08 from:
# /content/hyperparams.yaml
# yamllint disable
# TUNED HPARS
# DATASET HPARS
# band-pass filtering cut-off frequencies
fmin: 0.18 # @orion_step1: --fmin~"uniform(0.1, 5, precision=2)"
fmax: 21.9 # @orion_step1: --fmax~"uniform(20.0, 50.0, precision=3)"
# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class
# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long
# dataset interval starts from 2
# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)
tmin: 0.
tmax: 4.0 # @orion_step1: --tmax~"uniform(1.0, 4.0, precision=2)"
# number of steps used when selecting adjacent channels from a seed channel (default at Cz)
n_steps_channel_selection: 3 # @orion_step1: --n_steps_channel_selection~"uniform(1, 3,discrete=True)"
# We here specify how to perfom test:
# - If test_with: 'last' we perform test with the latest model.
# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_key)
# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.
# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.
test_with: best   # @orio_step1: --test_with~"choices(['last', 'best'])"
test_key: acc   # Possible opts: "loss", "f1", "auc", "acc"

# TRAINING HPARS
# checkpoints to average
avg_models: 1 # @orion_step1: --avg_models~"uniform(1, 15,discrete=True)"
number_of_epochs: 468 # @orion_step1: --number_of_epochs~"uniform(250, 1000, discrete=True)"
lr: 0.005 # @orion_step1: --lr~"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])"
batch_size_exponent: 6 # tuned 

# DATA AUGMENTATION
# cutcat (disabled when min_num_segments=max_num_segments=1)
max_num_segments: 3 # @orion_step2: --max_num_segments~"uniform(2, 6, discrete=True)"
cutcat: &id001 !new:speechbrain.augment.time_domain.CutCat
  min_num_segments: 2
  max_num_segments: 3
# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)
amp_delta: 0.05499 # @orion_step2: --amp_delta~"uniform(0.0, 0.5)"
# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)
shift_delta_: 1 # orion_step2: --shift_delta_~"uniform(0, 25, discrete=True)"
# injection of gaussian white noise
snr_white_low: 10.0 # @orion_step2: --snr_white_low~"uniform(0.0, 15, precision=2)"
snr_white_delta: 8.33 # @orion_step2: --snr_white_delta~"uniform(5.0, 20.0, precision=3)"
# pipeline
repeat_augment: 1 # @orion_step1: --repeat_augment 0
augment: !new:speechbrain.augment.augmenter.Augmenter
  parallel_augment: true
  concat_original: true
  parallel_augment_fixed_bs: true
  repeat_augment: 1
  shuffle_augmentations: true
  min_augmentations: 4
  max_augmentations: 4
  augmentations: [*id001, &id006 !new:speechbrain.augment.time_domain.RandAmp {amp_low: 0.94501,
      amp_high: 1.05499}, &id009 !new:speechbrain.augment.freq_domain.RandomShift {
      min_shift: &id007 !apply:math.floor [-1.25], max_shift: &id008 !apply:math.floor [
        1.25], dim: 1}, &id010 !new:speechbrain.augment.time_domain.AddNoise {snr_low: 10.0,
      snr_high: 18.33

# DATA NORMALIZATION
}]

# MODEL
# branch 1
cnn_temporal_kernels_b1: 6 # @orion_step1: --cnn_temporal_kernels_b1~"uniform(2, 8, discrete=True)"
cnn_temporal_kernelsize_b1: 26 # @orion_step1: --cnn_temporal_kernelsize_b1~"uniform(8, 32, discrete=True)"
cnn_sep_temporal_kernelsize_b1: 9 # @orion_step1: --cnn_sep_temporal_kernelsize_b1~"uniform(8, 32, discrete=True)"
dropout_b1: 0.  # @orion_step1: --dropout_b1~"uniform(0.0, 0.5)"
# branch 2
cnn_temporal_kernels_b2: 7 # @orion_step1: --cnn_temporal_kernels_b2~"uniform(4, 16, discrete=True)"
cnn_temporal_kernelsize_b2: 16 # @orion_step1: --cnn_temporal_kernelsize_b2~"uniform(16, 64, discrete=True)"
cnn_sep_temporal_kernelsize_b2: 32 # @orion_step1: --cnn_sep_temporal_kernelsize_b2~"uniform(8, 32, discrete=True)"
dropout_b2: 0.1  # @orion_step1: --dropout_b2~"uniform(0.0, 0.5)"
# branch 3
cnn_temporal_kernels_b3: 30 # @orion_step1: --cnn_temporal_kernels_b3~"uniform(8, 32, discrete=True)"
cnn_temporal_kernelsize_b3: 57 # @orion_step1: --cnn_temporal_kernelsize_b3~"uniform(32, 128, discrete=True)"
cnn_sep_temporal_kernelsize_b3: 14 # @orion_step1: --cnn_sep_temporal_kernelsize_b3~"uniform(8, 32, discrete=True)"
dropout_b3: 0.2  # @orion_step1: --dropout_b3~"uniform(0.0, 0.5)"
# common parameters
cnn_depth_multiplier: 2
cnn_depth_max_norm: 1
cnn_depth_pool_size: 4
cnn_sep_temporal_multiplier: 1
cnn_sep_temporal_pool_size: 5 # @orion_step1: --cnn_sep_temporal_pool_size~"uniform(1, 8, discrete=True)"
cnn_pool_type: avg
dense_n_neurons: 4
dense_max_norm: 0.25
activation_type: elu

##############################################################################################################
# OTHER HPARS
seed: 2171
__set_torchseed: !apply:torch.manual_seed [2171]

# DIRECTORIES
data_folder: /content/data/BNCI2014001
                           #'/path/to/dataset'. The dataset will be automatically downloaded in this folder
cached_data_folder: /content/data
                                 #'path/to/pickled/dataset'
output_folder: /content/results/full-experiment/BNCI2014001/run7/2171
                            #'path/to/results'

# DATASET HPARS
# Defining the MOABB dataset.
dataset: !new:moabb.datasets.BNCI2014001
save_prepared_dataset: true # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards
data_iterator_name: leave-one-session-out
target_subject_idx: 6
target_session_idx: 1
events_to_load:      # all events will be loaded
original_sample_rate: 250 # Original sampling rate provided by dataset authors
sample_rate: 125 # Target sampling rate (Hz)
n_classes: 4
T: 500
C: 22

# METRICS
f1: &id002 !name:sklearn.metrics.f1_score
  average: macro
acc: &id003 !name:sklearn.metrics.balanced_accuracy_score
cm: &id004 !name:sklearn.metrics.confusion_matrix

# TRAINING HPARS
metrics:
  f1: *id002
  acc: *id003
  cm: *id004
n_train_examples: 232  # it will be replaced in the train script
# Learning rate scheduling (cyclic learning rate is used here)
max_lr: 0.005     # Upper bound of the cycle (max value of the lr)
base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)
step_size_multiplier: 5 #from 2 to 8
step_size: &id005 !apply:round
- 18.125
lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler
  base_lr: 0.00000001
  max_lr: 0.005
  step_size: *id005
label_smoothing: 0.0
loss: !name:speechbrain.nnet.losses.nll_loss
  label_smoothing: 0.0
optimizer: !name:torch.optim.AdamW
  lr: 0.005
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
                                                               # epoch counter
  limit: 468
batch_size: 64
valid_ratio: 0.2

# DATA AUGMENTATION
rand_amp: *id006
shift_delta: 0.01                       # 0.250 # 0.-0.25 with steps of 0.01
min_shift: *id007
max_shift: *id008
time_shift: *id009
snr_white_high: 18.33
add_noise_white: *id010
dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)
normalize: !name:speechbrain.processing.signal_processing.mean_std_norm
  dims: 1

# MODEL
input_shape: &id011 [null, 500, 22, null]
    # branch 1

model: !new:models.MyModel.EEGNetFusion
  input_shape: *id011
  cnn_temporal_kernels_b1: 6
  cnn_temporal_kernelsize_b1: [26, 1]
  cnn_sep_temporal_kernelsize_b1: [9, 1]
  dropout_b1: 0.
    # branch 2
  cnn_temporal_kernels_b2: 7
  cnn_temporal_kernelsize_b2: [16, 1]
  cnn_sep_temporal_kernelsize_b2: [32, 1]
  dropout_b2: 0.1
    # branch 3
  cnn_temporal_kernels_b3: 30
  cnn_temporal_kernelsize_b3: [57, 1]
  cnn_sep_temporal_kernelsize_b3: [14, 1]
  dropout_b3: 0.2
    # common parameters
  cnn_depth_multiplier: 2
  cnn_depth_max_norm: 1
  cnn_depth_pool_size: [4, 1]
  cnn_sep_temporal_multiplier: 1
  cnn_sep_temporal_pool_size: [5, 1]
  cnn_pool_type: avg
  dense_n_neurons: 4
  dense_max_norm: 0.25
  activation_type: elu
